{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 1 \n",
    "# sentence1 = \"The bright and radiant sun shone down upon the lush green fields filling them with warmth and life\"\n",
    "# sentence2 = \"The brilliant and glowing star cast its brilliant light onto the verdant meadows imbuing them with vitality and heat\"\n",
    "# Spacy similarity score: 0.9305430865896751\n",
    "# Jaccard similarity score: 0.1724137931034483\n",
    "# Cosine similarity score: 0.6741998624632421\n",
    "# Weighted similarity score: 0.6518356537185205\n",
    "# Average similarity score: 0.7150130948423727 0.6072480989687216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 2 fake\n",
    "# sentence1 = \"Machine Learning (ML) is a subfield of Artificial Intelligence (AI) that involves the development of algorithms and models that enable computers to learn from data and make predictions or decisions without being explicitly programmed. In other words, ML involves training a machine to identify patterns in data and use those patterns to make predictions or decisions.\"\n",
    "# sentence2 = \"Environmental Studies(EVS full form) is the study of the environment, which refers to the surroundings in which all live. Everything that directly or indirectly promotes human survival is considered part of the environment. Plants, animals, and soil, water, and air are examples of living and non-living components. The living components interact with their surroundings and adjust to the changing conditions.\"\n",
    "# Spacy similarity score: 0.9039110083754578\n",
    "# Jaccard similarity score: 0.09876543209876543\n",
    "# Cosine similarity score: 0.4720587952499558\n",
    "# Weighted similarity score: 0.5759968928673497\n",
    "# Average similarity score: 0.6430923575570741 0.5126830321478821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tets 3\n",
    "# sentence1 = \"Machine Learning (ML) is a subfield of Artificial Intelligence (AI) that involves the development of algorithms and models that enable computers to learn from data and make predictions or decisions without being explicitly programmed. In other words, ML involves training a machine to identify patterns in data and use those patterns to make predictions or decisions.\"\n",
    "# sentence2 = \"Machine Learning can also refer to the process of training a machine learning model on a dataset to make predictions or decisions. This involves selecting an appropriate algorithm, preparing the data, and tuning the parameters of the model to achieve the desired level of accuracy. The ultimate goal of ML is to create models that can generalize well to new, unseen data and make accurate predictions or decisions in real-world scenarios.\"\n",
    "# Spacy similarity score: 0.952206070069469\n",
    "# Jaccard similarity score: 0.28\n",
    "# Cosine similarity score: 0.7046642634176443\n",
    "# Weighted similarity score: 0.7010358877182633\n",
    "# Average similarity score: 0.7570530602240524 0.6594765553013442"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy similarity score: 0.8816250196351941\n",
      "Cosine similarity score: 0.30860669992418377\n",
      "Jaccard similarity score: 0.09523809523809523\n",
      "Weighted similarity score: 0.5311052783738623\n",
      "average  0.45414377329283384 0.4438937774573262\n"
     ]
    }
   ],
   "source": [
    "'''snippet 1'''\n",
    "import spacy\n",
    "\n",
    "# Load the pre-trained model with word vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Define the two sentences to compare\n",
    "# sentence1 = \"Machine Learning (ML) is a subfield of Artificial Intelligence (AI) that involves the development of algorithms and models that enable computers to learn from data and make predictions or decisions without being explicitly programmed. In other words, ML involves training a machine to identify patterns in data and use those patterns to make predictions or decisions.\"\n",
    "# sentence2 = \"Machine Learning can also refer to the process of training a machine learning model on a dataset to make predictions or decisions. This involves selecting an appropriate algorithm, preparing the data, and tuning the parameters of the model to achieve the desired level of accuracy. The ultimate goal of ML is to create models that can generalize well to new, unseen data and make accurate predictions or decisions in real-world scenarios.\"\n",
    "sentence1 = \"Machine Learning (ML) is a subfield of Artificial Intelligence (AI) that involves the development of algorithms and models that enable computers to learn from data and make predictions or decisions without being explicitly programmed. In other words, ML involves training a machine to identify patterns in data and use those patterns to make predictions or decisions.\"\n",
    "sentence2 = \"Environmental Studies(EVS full form) is the study of the environment, which refers to the surroundings in which all live. Everything that directly or indirectly promotes human survival is considered part of the environment. Plants, animals, and soil, water, and air are examples of living and non-living components. The living components interact with their surroundings and adjust to the changing conditions.\"\n",
    "\n",
    "# Process the sentences to create document objects\n",
    "doc1 = nlp(sentence1)\n",
    "doc2 = nlp(sentence2)\n",
    "\n",
    "# Compute the average word vectors for each sentence\n",
    "vector1 = sum([token.vector for token in doc1])/len(doc1)\n",
    "vector2 = sum([token.vector for token in doc2])/len(doc2)\n",
    "\n",
    "# Calculate the cosine similarity between the sentence vectors\n",
    "from scipy.spatial.distance import cosine\n",
    "similarity_score = 1 - cosine(vector1, vector2)\n",
    "\n",
    "# Compute the sentence similarity using spaCy's similarity function\n",
    "spacy_similarity = doc1.similarity(doc2)\n",
    "\n",
    "# Calculate the final similarity score as a weighted average of the sentence and word vector similarity\n",
    "final_similarity_score = 0.5 * similarity_score + 0.5 * spacy_similarity\n",
    "\n",
    "# Print the final similarity score\n",
    "print(\"Final similarity score:\", final_similarity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "def similarity(request, id):\n",
    "    document = get_object_or_404(Document, id=id)\n",
    "    file_docs = []\n",
    "    file2_docs = []\n",
    "    avg_sims = []\n",
    "    with open ('media/' + document.document.name) as f:\n",
    "        tokens = sent_tokenize(f.read())\n",
    "        for line in tokens:\n",
    "            file_docs.append(line)\n",
    "            \n",
    "    length_doc1 = len(file_docs)\n",
    "\n",
    "    gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "                for text in file_docs]\n",
    "\n",
    "    dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "    corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "    tf_idf = gensim.models.TfidfModel(corpus)\n",
    "    sims = gensim.similarities.Similarity('workdir/',tf_idf[corpus],\n",
    "                                        num_features=len(dictionary))\n",
    "\n",
    "    with open ('media/' + document.document2.name) as f:\n",
    "        tokens = sent_tokenize(f.read())\n",
    "        for line in tokens:\n",
    "            file2_docs.append(line)\n",
    "            \n",
    "    for line in file2_docs:\n",
    "        query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "        query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "        print('Comparing Result:', sims[query_doc_tf_idf]) \n",
    "        sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "        avg = sum_of_sims / len(file_docs)\n",
    "        print(f'avg: {sum_of_sims / len(file_docs)}')\n",
    "        avg_sims.append(avg)  \n",
    "    total_avg = np.sum(avg_sims, dtype=np.float)\n",
    "    print(total_avg)\n",
    "    percentage_of_similarity = round(float(total_avg) * 100)\n",
    "    if percentage_of_similarity >= 100:\n",
    "        percentage_of_similarity = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def similarity(string1, string2):\n",
    "    file_docs = string1\n",
    "    file2_docs = string2\n",
    "    avg_sims = []\n",
    "\n",
    "    gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "                for text in file_docs]\n",
    "\n",
    "    dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "    corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "    tf_idf = gensim.models.TfidfModel(corpus)\n",
    "    sims = gensim.similarities.Similarity(tf_idf[corpus],\n",
    "                                        num_features=len(dictionary))\n",
    "    print(sims)\n",
    "\n",
    "    for line in file2_docs:\n",
    "        query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "        query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "        query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "        # print('Comparing Result:', sims[query_doc_tf_idf]) \n",
    "        sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "        avg = sum_of_sims / len(file_docs)\n",
    "        # print(f'avg: {sum_of_sims / len(file_docs)}')\n",
    "        avg_sims.append(avg)  \n",
    "    total_avg = np.sum(avg_sims, dtype=np.float)\n",
    "    print(total_avg)\n",
    "    percentage_of_similarity = round(float(total_avg))\n",
    "   \n",
    "    return percentage_of_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WMD similarity between 'The quick brown fox jumps over the lazy dog.' and 'The lazy dog is jumped over by the quick brown fox.': 0.1574\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def similarityx(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes Word Mover's Distance (WMD) similarity between two sentences.\n",
    "    \"\"\"\n",
    "    s1_doc = nlp(s1)\n",
    "    s2_doc = nlp(s2)\n",
    "    return 1 - s1_doc.similarity(s2_doc)\n",
    "\n",
    "s1 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "s2 = \"The lazy dog is jumped over by the quick brown fox.\"\n",
    "\n",
    "wmd_sim = similarityx(s1, s2)\n",
    "print(f\"WMD similarity between '{s1}' and '{s2}': {wmd_sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score is : 0.4192897\n"
     ]
    }
   ],
   "source": [
    "# sentence1 = \"Machine Learning (ML) is a subfield of Artificial Intelligence (AI) that involves the development of algorithms and models that enable computers to learn from data and make predictions or decisions without being explicitly programmed. In other words, ML involves training a machine to identify patterns in data and use those patterns to make predictions or decisions.\"\n",
    "# sentence2 = \"Environmental Studies(EVS full form) is the study of the environment, which refers to the surroundings in which all live. Everything that directly or indirectly promotes human survival is considered part of the environment. Plants, animals, and soil, water, and air are examples of living and non-living components. The living components interact with their surroundings and adjust to the changing conditions.\"\n",
    "# sentence1 = \"Machine Learning (ML) is a subfield of Artificial Intelligence (AI) that involves the development of algorithms and models that enable computers to learn from data and make predictions or decisions without being explicitly programmed. In other words, ML involves training a machine to identify patterns in data and use those patterns to make predictions or decisions.\"\n",
    "# sentence2 = \"Machine Learning can also refer to the process of training a machine learning model on a dataset to make predictions or decisions. This involves selecting an appropriate algorithm, preparing the data, and tuning the parameters of the model to achieve the desired level of accuracy. The ultimate goal of ML is to create models that can generalize well to new, unseen data and make accurate predictions or decisions in real-world scenarios.\"\n",
    "sentence1 = \"computer science is a type of artificial intelligence (AI) that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. Machine learning algorithms use historical data as input to predict new output values.\"\n",
    "sentence2 = \"computer science is the study of computers and computational systems. Unlike electrical and computer engineers, computer scientists deal mostly with software and software systems; this includes their theory, design, development, and application.\"\n",
    "# sentence1 = \"The bright and radiant sun shone down upon the lush green fields filling them with warmth and life\"\n",
    "# sentence2 = \"The brilliant and glowing star cast its brilliant light onto the verdant meadows imbuing them with vitality and heat\"\n",
    "\n",
    "result = similarity(sentence1,sentence2)\n",
    "print(\"Score is :\",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'distance'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects and codes\\interview\\test.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X11sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m sentences1 \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mMobile devices have transformed the way we consume media, with many people now using smartphones and tablets to watch videos, listen to music, and browse the web.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X11sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mArtificial intelligence is rapidly transforming many industries, from healthcare to finance to transportation.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X11sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mThe COVID-19 pandemic has highlighted the importance of public health and the need for effective healthcare systems.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X11sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mClimate change is a global challenge that requires urgent action from individuals, governments, and businesses.\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m sentences2 \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mThe widespread adoption of mobile devices has led to a shift in how we access and consume media, with many people now preferring to watch videos, listen to music, and browse the internet on their smartphones and tablets.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X11sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mAI is changing the game in industries ranging from healthcare to finance to transportation.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X11sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mEffective healthcare systems are critical for public health, as demonstrated by the challenges posed by COVID-19.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X11sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mAddressing climate change will require cooperation and action from individuals, governments, and businesses worldwide.\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X11sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m df \u001b[39m=\u001b[39m compare_sentence_pairs(sentences1, sentences2, preprocess\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X11sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mprint\u001b[39m(df\u001b[39m.\u001b[39mhead())\n",
      "\u001b[1;32md:\\Projects and codes\\interview\\test.ipynb Cell 9\u001b[0m in \u001b[0;36mcompare_sentence_pairs\u001b[1;34m(sentences1, sentences2, preprocess)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X11sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m cosine_score \u001b[39m=\u001b[39m cosine(sent1, sent2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X11sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m levenshtein_score \u001b[39m=\u001b[39m levenshtein\u001b[39m.\u001b[39mnormalized_distance(sent1, sent2)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X11sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m lcs_score \u001b[39m=\u001b[39m lcsseq(sent1, sent2)\u001b[39m.\u001b[39;49mdistance\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X11sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m ratcliff_obershelp_score \u001b[39m=\u001b[39m ratcliff_obershelp(sent1, sent2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m results\u001b[39m.\u001b[39mappend([i, j, sent1, sent2, jaccard_score, sorensen_dice_score, cosine_score, levenshtein_score, lcs_score, ratcliff_obershelp_score])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'distance'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from textdistance import jaccard, sorensen_dice, cosine, levenshtein, lcsseq, ratcliff_obershelp\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses a string by converting it to lowercase, removing punctuation, and removing stop words.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "def compare_sentence_pairs(sentences1, sentences2, preprocess=False):\n",
    "    \"\"\"\n",
    "    Compares pairs of sentences using seven similarity metrics and returns the results in a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i, sent1 in enumerate(sentences1):\n",
    "        for j, sent2 in enumerate(sentences2):\n",
    "            if preprocess:\n",
    "                sent1 = preprocess_text(sent1)\n",
    "                sent2 = preprocess_text(sent2)\n",
    "            jaccard_score = jaccard(sent1, sent2)\n",
    "            sorensen_dice_score = sorensen_dice(sent1, sent2)\n",
    "            cosine_score = cosine(sent1, sent2)\n",
    "            levenshtein_score = levenshtein.normalized_distance(sent1, sent2)\n",
    "            lcs_score = lcsseq(sent1, sent2).distance\n",
    "            ratcliff_obershelp_score = ratcliff_obershelp(sent1, sent2)\n",
    "            results.append([i, j, sent1, sent2, jaccard_score, sorensen_dice_score, cosine_score, levenshtein_score, lcs_score, ratcliff_obershelp_score])\n",
    "    columns = [\"sent1_id\", \"sent2_id\", \"sent1\", \"sent2\", \"jaccard\", \"sorensen_dice\", \"cosine\", \"levenshtein\", \"lcs\", \"ratcliff_obershelp\"]\n",
    "    df = pd.DataFrame(results, columns=columns)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "sentences1 = [\"Mobile devices have transformed the way we consume media, with many people now using smartphones and tablets to watch videos, listen to music, and browse the web.\",\n",
    "              \"Artificial intelligence is rapidly transforming many industries, from healthcare to finance to transportation.\",\n",
    "              \"The COVID-19 pandemic has highlighted the importance of public health and the need for effective healthcare systems.\",\n",
    "              \"Climate change is a global challenge that requires urgent action from individuals, governments, and businesses.\"]\n",
    "sentences2 = [\"The widespread adoption of mobile devices has led to a shift in how we access and consume media, with many people now preferring to watch videos, listen to music, and browse the internet on their smartphones and tablets.\",\n",
    "              \"AI is changing the game in industries ranging from healthcare to finance to transportation.\",\n",
    "              \"Effective healthcare systems are critical for public health, as demonstrated by the challenges posed by COVID-19.\",\n",
    "              \"Addressing climate change will require cooperation and action from individuals, governments, and businesses worldwide.\"]\n",
    "\n",
    "df = compare_sentence_pairs(sentences1, sentences2, preprocess=True)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance\n",
    "\n",
    "def lcsseq(sentence1, sentence2):\n",
    "    # Tokenize the input sentences\n",
    "    words1 = sentence1.split()\n",
    "    words2 = sentence2.split()\n",
    "\n",
    "    # Calculate the LCS distance between the two lists of words\n",
    "    lcs = textdistance.lcsseq(words1, words2)\n",
    "    lcs_length = len(lcs)\n",
    "    \n",
    "    return lcs_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.vectors.Vectors' object has no attribute 'vectors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects and codes\\interview\\test.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X13sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m sentence1 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mThe quick brown fox jumps over the lazy dog\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X13sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m sentence2 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mA brown fox jumps over a lazy dog\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m similarity \u001b[39m=\u001b[39m sif_similarity(sentence1, sentence2, word_embeddings)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X13sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSIF similarity:\u001b[39m\u001b[39m\"\u001b[39m, similarity)\n",
      "\u001b[1;32md:\\Projects and codes\\interview\\test.ipynb Cell 11\u001b[0m in \u001b[0;36msif_similarity\u001b[1;34m(sentence1, sentence2, word_embeddings)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msif_similarity\u001b[39m(sentence1, sentence2, word_embeddings):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X13sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m# Calculate the sentence embeddings using SIF\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     sentence1_embedding \u001b[39m=\u001b[39m sif_sentence_embedding(sentence1, word_embeddings)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X13sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     sentence2_embedding \u001b[39m=\u001b[39m sif_sentence_embedding(sentence2, word_embeddings)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m# Calculate the cosine similarity between the embeddings\u001b[39;00m\n",
      "\u001b[1;32md:\\Projects and codes\\interview\\test.ipynb Cell 11\u001b[0m in \u001b[0;36msif_sentence_embedding\u001b[1;34m(sentence, word_embeddings, alpha)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m probs \u001b[39m=\u001b[39m {word: freq \u001b[39m/\u001b[39m total_words \u001b[39mfor\u001b[39;00m word, freq \u001b[39min\u001b[39;00m freqs\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Calculate the sentence embedding\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m embeddings \u001b[39m=\u001b[39m [word_embeddings[word]\u001b[39m.\u001b[39mvector \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m word_embeddings \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mzeros((word_embeddings\u001b[39m.\u001b[39mvectors\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],)) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Calculate the weighted average of the embeddings\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m weights \u001b[39m=\u001b[39m [alpha \u001b[39m/\u001b[39m (alpha \u001b[39m+\u001b[39m probs[word]) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words]\n",
      "\u001b[1;32md:\\Projects and codes\\interview\\test.ipynb Cell 11\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m probs \u001b[39m=\u001b[39m {word: freq \u001b[39m/\u001b[39m total_words \u001b[39mfor\u001b[39;00m word, freq \u001b[39min\u001b[39;00m freqs\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Calculate the sentence embedding\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m embeddings \u001b[39m=\u001b[39m [word_embeddings[word]\u001b[39m.\u001b[39mvector \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m word_embeddings \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mzeros((word_embeddings\u001b[39m.\u001b[39;49mvectors\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],)) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Calculate the weighted average of the embeddings\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m weights \u001b[39m=\u001b[39m [alpha \u001b[39m/\u001b[39m (alpha \u001b[39m+\u001b[39m probs[word]) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.vectors.Vectors' object has no attribute 'vectors'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "stopwords = stopwords.words(\"english\")\n",
    "\n",
    "def sif_sentence_embedding(sentence, word_embeddings, alpha=1e-3):\n",
    "    words = [word.text.lower() for word in nlp(sentence)]\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    # Get the word frequencies in the corpus\n",
    "    freqs = Counter(words)\n",
    "    # Calculate the total number of words in the corpus\n",
    "    total_words = sum(freqs.values())\n",
    "    # Calculate the probabilities of each word in the corpus\n",
    "    probs = {word: freq / total_words for word, freq in freqs.items()}\n",
    "    # Calculate the sentence embedding\n",
    "    embeddings = [word_embeddings[word].vector if word in word_embeddings else np.zeros((word_embeddings.vectors.shape[1],)) for word in words]\n",
    "    # Calculate the weighted average of the embeddings\n",
    "    weights = [alpha / (alpha + probs[word]) for word in words]\n",
    "    embedding = np.average(embeddings, axis=0, weights=weights)\n",
    "    return embedding\n",
    "\n",
    "def sif_similarity(sentence1, sentence2, word_embeddings):\n",
    "    # Calculate the sentence embeddings using SIF\n",
    "    sentence1_embedding = sif_sentence_embedding(sentence1, word_embeddings)\n",
    "    sentence2_embedding = sif_sentence_embedding(sentence2, word_embeddings)\n",
    "    # Calculate the cosine similarity between the embeddings\n",
    "    similarity = np.dot(sentence1_embedding, sentence2_embedding) / (np.linalg.norm(sentence1_embedding) * np.linalg.norm(sentence2_embedding))\n",
    "    return similarity\n",
    "\n",
    "# Example usage\n",
    "word_embeddings = nlp.vocab.vectors\n",
    "sentence1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "sentence2 = \"A brown fox jumps over a lazy dog\"\n",
    "similarity = sif_similarity(sentence1, sentence2, word_embeddings)\n",
    "print(\"SIF similarity:\", similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin:  40%|████      | 178M/440M [04:39<06:54, 635kB/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from pytorch checkpoint file for 'C:\\Users\\Hp/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\pytorch_model.bin' at 'C:\\Users\\Hp/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:415\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 415\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(checkpoint_file, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:815\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    814\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 815\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:1051\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1050\u001b[0m typed_storage \u001b[39m=\u001b[39m deserialized_objects[key]\n\u001b[1;32m-> 1051\u001b[0m typed_storage\u001b[39m.\u001b[39;49m_untyped_storage\u001b[39m.\u001b[39;49m_set_from_file(\n\u001b[0;32m   1052\u001b[0m     f, offset, f_should_read_directly,\n\u001b[0;32m   1053\u001b[0m     torch\u001b[39m.\u001b[39;49m_utils\u001b[39m.\u001b[39;49m_element_size(typed_storage\u001b[39m.\u001b[39;49mdtype))\n\u001b[0;32m   1054\u001b[0m \u001b[39mif\u001b[39;00m offset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: unexpected EOF, expected 6038992 more bytes. The file might be corrupted.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:419\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(checkpoint_file) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 419\u001b[0m     \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39;49mread(\u001b[39m7\u001b[39;49m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    420\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    421\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou seem to have cloned a repository without having git-lfs installed. Please install \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    422\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgit-lfs and run `git lfs install` followed by `git lfs pull` in the folder \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    423\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myou cloned.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_decode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 2324: character maps to <undefined>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects and codes\\interview\\test.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(model_name, num_labels\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Set the device to GPU if available, otherwise use CPU\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/test.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    470\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 471\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    472\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    473\u001b[0m     )\n\u001b[0;32m    474\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    475\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    476\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:2429\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2426\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n\u001b[0;32m   2427\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sharded \u001b[39mand\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2428\u001b[0m         \u001b[39m# Time to load the checkpoint\u001b[39;00m\n\u001b[1;32m-> 2429\u001b[0m         state_dict \u001b[39m=\u001b[39m load_state_dict(resolved_archive_file)\n\u001b[0;32m   2431\u001b[0m     \u001b[39m# set dtype to instantiate the model under:\u001b[39;00m\n\u001b[0;32m   2432\u001b[0m     \u001b[39m# 1. If torch_dtype is not None, we use that dtype\u001b[39;00m\n\u001b[0;32m   2433\u001b[0m     \u001b[39m# 2. If torch_dtype is \"auto\", we auto-detect dtype from the loaded state_dict, by checking its first\u001b[39;00m\n\u001b[0;32m   2434\u001b[0m     \u001b[39m#    weights entry that is of a floating type - we assume all floating dtype weights are of the same dtype\u001b[39;00m\n\u001b[0;32m   2435\u001b[0m     \u001b[39m# we also may have config.torch_dtype available, but we won't rely on it till v5\u001b[39;00m\n\u001b[0;32m   2436\u001b[0m     dtype_orig \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:431\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m(checkpoint_file)\u001b[0m\n\u001b[0;32m    426\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    427\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to locate the file \u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_file\u001b[39m}\u001b[39;00m\u001b[39m which is necessary to load this pretrained \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmodel. Make sure you have saved the model properly.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m             ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mUnicodeDecodeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m--> 431\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    432\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to load weights from pytorch checkpoint file for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    433\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mat \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mcheckpoint_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    434\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    435\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to load weights from pytorch checkpoint file for 'C:\\Users\\Hp/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\pytorch_model.bin' at 'C:\\Users\\Hp/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\0a6aa9128b6194f4f3c4db429b6cb4891cdb421b\\pytorch_model.bin'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define a function to calculate the similarity between two sentences\n",
    "def bert_similarity(sentence1, sentence2):\n",
    "    # Tokenize the input sentences\n",
    "    inputs = tokenizer(sentence1, sentence2, padding=True, truncation=True, return_tensors='pt')\n",
    "    inputs = inputs.to(device)\n",
    "    # Make a forward pass through the model\n",
    "    outputs = model(**inputs)\n",
    "    # Get the logits and calculate the sigmoid to obtain the similarity score\n",
    "    logits = outputs.logits\n",
    "    similarity = torch.sigmoid(logits).item()\n",
    "    return similarity\n",
    "\n",
    "# Test the function\n",
    "sentence1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "sentence2 = \"A brown fox jumps over a lazy dog\"\n",
    "similarity = bert_similarity(sentence1, sentence2)\n",
    "print(\"BERT similarity:\", similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "027efd7095115064582efab50921964d678032cf36f1ec215c675cf4ffc5e6b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
