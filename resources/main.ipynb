{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core_web_lg \n",
      "\n",
      "Spacy similarity score: 0.9514847092758087\n",
      "Cosine similarity score: 0.7483314773547882\n",
      "Jaccard similarity score: 0.2916666666666667\n",
      "Weighted similarity score: 0.7129086501088621\n",
      "Gensim similarity score: 0.8614414\n",
      "average  0.7131665754462398\n",
      "\n",
      "\n",
      "Answer score is : 0.7807980301971439 \n",
      "\n",
      "\n",
      "core_web_md \n",
      "\n",
      "Spacy similarity score: 0.9546089382680001\n",
      "Cosine similarity score: 0.7483314773547882\n",
      "Jaccard similarity score: 0.2916666666666667\n",
      "Weighted similarity score: 0.7144707646049577\n",
      "Gensim similarity score: 0.8614414\n",
      "average  0.7141038441438973\n",
      "\n",
      "\n",
      "Answer score is : 0.7813838231331798 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textdistance import jaccard, sorensen_dice, cosine, levenshtein, lcsseq, ratcliff_obershelp\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import textdistance\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for i in range(0,2):\n",
    "    \n",
    "    # Load the Spacy model and create a TfidfVectorizer\n",
    "    # nlp = spacy.load('en_core_web_lg') if i == 0 else spacy.load(\"en_core_web_md\")\n",
    "    # print('en_core_web_lg' if i == 0 else 'en_core_web_md', '\\n')\n",
    "    nlp = spacy.load('en_core_web_lg' if i == 0 else 'en_core_web_md')\n",
    "    print(nlp.meta['name'], '\\n')\n",
    "\n",
    "\n",
    "    # Define two sample sentences\n",
    "    # sentence1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "    # sentence2 = \"A quick brown dog jumps over the lazy cat\"\n",
    "    sentence1 = \"Machine Learning (ML) is a subfield of Artificial Intelligence (AI) that involves the development of algorithms and models that enable computers to learn from data and make predictions or decisions without being explicitly programmed. In other words, ML involves training a machine to identify patterns in data and use those patterns to make predictions or decisions.\"\n",
    "    sentence2 = \"Machine Learning can also refer to the process of training a machine learning model on a dataset to make predictions or decisions. This involves selecting an appropriate algorithm, preparing the data, and tuning the parameters of the model to achieve the desired level of accuracy. The ultimate goal of ML is to create models that can generalize well to new, unseen data and make accurate predictions or decisions in real-world scenarios.\"\n",
    "    # sentence1 = \"Machine Learning (ML) is a subfield of Artificial Intelligence (AI) that involves the development of algorithms and models that enable computers to learn from data and make predictions or decisions without being explicitly programmed. In other words, ML involves training a machine to identify patterns in data and use those patterns to make predictions or decisions.\"\n",
    "    # sentence2 = \"Environmental Studies(EVS full form) is the study of the environment, which refers to the surroundings in which all live. Everything that directly or indirectly promotes human survival is considered part of the environment. Plants, animals, and soil, water, and air are examples of living and non-living components. The living components interact with their surroundings and adjust to the changing conditions.\"\n",
    "    # sentence1 = \"computer science is a type of artificial intelligence (AI) that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. Machine learning algorithms use historical data as input to predict new output values.\"\n",
    "    # sentence2 = \"computer science is the study of computers and computational systems. Unlike electrical and computer engineers, computer scientists deal mostly with software and software systems; this includes their theory, design, development, and application.\"\n",
    "    # sentence1 = \"The bright and radiant sun shone down upon the lush green fields filling them with warmth and life\"\n",
    "    # sentence2 = \"The brilliant and glowing star cast its brilliant light onto the verdant meadows imbuing them with vitality and heat\"\n",
    "    # sentence1 = \"Social media has revolutionized the way we communicate and interact with each other.\"\n",
    "    # sentence2 = \"The rise of social media has transformed the landscape of communication and interpersonal relationships.\"\n",
    "    # sentence1 =  \"Artificial intelligence (AI) is a field of computer science that focuses on creating machines that can perform tasks that typically require human intelligence.\"\n",
    "    # sentence2 = \"AI involves developing algorithms and models that enable machines to perform intelligent tasks, such as natural language processing and computer vision.\"\n",
    "    # sentence1 = \"The internet has become an essential tool for communication, research, and commerce in the modern era.\"\n",
    "    # sentence2 = \"In today's society, the internet plays a critical role in connecting people, providing information, and facilitating economic transactions.\"\n",
    "    # sentence1 = \"The Earth's atmosphere consists of several layers that protect the planet from harmful radiation and provide the air we breathe.\"\n",
    "    # sentence2 = \"The Earth's atmosphere is composed of several layers that help shield the planet from harmful cosmic rays and provide the necessary gases for life to thrive.\"\n",
    "    # sentence1 = \"Mobile devices have transformed the way we consume media, with many people now using smartphones and tablets to watch videos, listen to music, and browse the web.\"\n",
    "    # sentence2 = \"The widespread adoption of mobile devices has led to a shift in how we access and consume media, with many people now preferring to watch videos, listen to music, and browse the internet on their smartphones and tablets.\"\n",
    "\n",
    "    \"\"\"Nearly similar but below one is more accurate\"\"\"\n",
    "    # Preprocess the sentences\n",
    "    # def preprocess_sentence(sentence):\n",
    "    #     # sentence = sentence.lower()\n",
    "    #     sentence_tokens = nlp(sentence.lower())\n",
    "    #     sentence_tokens = nltk.word_tokenize(sentence.lower())\n",
    "    #     # sentence_tokens = [token.lemma_ for token in sentence_tokens if not token.is_stop and not token.is_punct]\n",
    "    #     sentence_tokens = [token for token in sentence_tokens if token not in stop_words]\n",
    "    #     lemmatizer = WordNetLemmatizer()\n",
    "    #     sentence_tokens = [lemmatizer.lemmatize(token) for token in sentence_tokens]\n",
    "    #     sentence = \" \".join(sentence_tokens)\n",
    "    #     return sentence\n",
    "    \"\"\"End\"\"\"\n",
    "\n",
    "    \"\"\"More accurate one\"\"\"\n",
    "    def preprocess_sentence(text):\n",
    "        \n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        words = text.split()\n",
    "        words = [w for w in words if w not in stop_words]\n",
    "        return \" \".join(words)\n",
    "\n",
    "\n",
    "\n",
    "    processed_sentence1 = preprocess_sentence(sentence1)\n",
    "    processed_sentence2 = preprocess_sentence(sentence2)\n",
    "\n",
    "\n",
    "    # Compute Spacy similarity score\n",
    "    doc1 = nlp(processed_sentence1)\n",
    "    doc2 = nlp(processed_sentence2)\n",
    "    spacy_similarity_score = doc1.similarity(doc2)\n",
    "\n",
    "\n",
    "    # Compute TF-IDF vector representation for each sentence\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    sentence1_tfidf = vectorizer.fit_transform([processed_sentence1])\n",
    "    sentence2_tfidf = vectorizer.transform([processed_sentence2])\n",
    "\n",
    "\n",
    "    # Compute cosine similarity score using TF-IDF vectors\n",
    "    cosine_similarity_score = cosine_similarity(sentence1_tfidf, sentence2_tfidf)[0][0]\n",
    "\n",
    "\n",
    "    # test\n",
    "    ''' test for snippet 1 '''\n",
    "    \"\"\"use with try and expect\"\"\"\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        tokens1 = word_tokenize(processed_sentence1)\n",
    "        tokens2 = word_tokenize(processed_sentence2)\n",
    "            \n",
    "        # create word embeddings for each string\n",
    "        model = gensim.models.Word2Vec([tokens1, tokens2], min_count=2)\n",
    "        word_vectors = {word: model.wv[word] for word in model.wv.index_to_key}\n",
    "            \n",
    "        # calculate the average vector for each string\n",
    "        vector1 = np.mean([word_vectors[word] for word in tokens1 if word in word_vectors], axis=0)\n",
    "        vector2 = np.mean([word_vectors[word] for word in tokens2 if word in word_vectors], axis=0)\n",
    "            \n",
    "        # calculate cosine similarity between the vectors\n",
    "        sim_score = cosine_similarity([vector1], [vector2])[0][0]\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        sim_score = weighted_score\n",
    "\n",
    "    \"\"\"end\"\"\"\n",
    "    # test\n",
    "\n",
    "\n",
    "    # Compute Jaccard similarity score using set intersection\n",
    "    # words1 = set(processed_sentence1.split())\n",
    "    # words2 = set(processed_sentence2.split())\n",
    "    # jaccard_similarity_score = len(words1.intersection(words2)) / len(words1.union(words2))\n",
    "\n",
    "    tokens1 = set(nltk.word_tokenize(sentence1.lower()))\n",
    "    tokens2 = set(nltk.word_tokenize(sentence2.lower()))\n",
    "\n",
    "    jaccard_similarity_score = len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))\n",
    "\n",
    "\n",
    "    # Weighted average of the similarity scores\n",
    "    weighted_score = (0.5 * spacy_similarity_score) + (0.3 * jaccard_similarity_score) + (0.2 * cosine_similarity_score)\n",
    "\n",
    "\n",
    "    # test\n",
    "    \"\"\"Useless genrates use less values\"\"\"\n",
    "    \"\"\"begin\"\"\"\n",
    "    # jaccard_score1 = jaccard(processed_sentence1, processed_sentence2)\n",
    "    # sorensen_dice_score1 = sorensen_dice(processed_sentence1, processed_sentence2)\n",
    "    # cosine_score1 = cosine(processed_sentence1, processed_sentence2)\n",
    "    # levenshtein_score1 = levenshtein.normalized_distance(processed_sentence1, processed_sentence2)\n",
    "    # ratcliff_obershelp_score1 = ratcliff_obershelp(processed_sentence1, processed_sentence2)\n",
    "    # edit_distance = textdistance.levenshtein.normalized_distance(sentence1, sentence2)\n",
    "\n",
    "    # print(\"Jaccard Index:\", jaccard_score1)\n",
    "    # print(\"SÃ¸rensen-Dice coefficient:\", sorensen_dice_score1)\n",
    "    # print(\"Cosine similarity:\", cosine_score1)\n",
    "    # print(\"Edit distance:\", edit_distance)\n",
    "    # print(\"Ratcliff/Obershelp similarity:\", ratcliff_obershelp_score1)\n",
    "    # print(\"Levenshtein distance:\", levenshtein_score1,\"\\n\\n\\n\")\n",
    "    \"\"\"end\"\"\"\n",
    "    # test\n",
    "\n",
    "\n",
    "    # Compute the average similarity score\n",
    "    average_similarity_score = (spacy_similarity_score + cosine_similarity_score + jaccard_similarity_score + weighted_score + sim_score) / 5\n",
    "    avg = spacy_similarity_score * .1 + jaccard_similarity_score*.2 + cosine_similarity_score*.2 + weighted_score*.2 + sim_score * .2\n",
    "    ###\n",
    "    avg1 = spacy_similarity_score * .15 + ( jaccard_similarity_score + cosine_similarity_score + ( weighted_score + sim_score ) * .15 ) / 2\n",
    "    # test\n",
    "\n",
    "    # test\n",
    "\n",
    "    # Print the similarity scores\n",
    "    print(\"Spacy similarity score:\", spacy_similarity_score)\n",
    "    print(\"Cosine similarity score:\", cosine_similarity_score)\n",
    "    print(\"Jaccard similarity score:\",jaccard_similarity_score)\n",
    "    print(\"Weighted similarity score:\", weighted_score)\n",
    "    print(\"Gensim similarity score:\", sim_score)\n",
    "    print(\"average \",average_similarity_score)\n",
    "    print(\"\\n\\nAnswer score is :\",avg1,\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a1', 'a2', 'a3', 'a4']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feron(question):\n",
    "    print(question)\n",
    "    return [\"a1\",\"a2\",\"a3\",\"a4\"]\n",
    "\n",
    "def getAnswers(questions,number):\n",
    "    \n",
    "    answers = []\n",
    "    answers = feron(questions[number])\n",
    "    return answers\n",
    "\n",
    "getAnswers([\"q1\",\"q2\",\"q3\",\"q4\"],1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import numpy as np\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScore(sentence1,sentence2):\n",
    "    \n",
    "    def preprocess_sentence(text):\n",
    "            \n",
    "            text = text.lower()\n",
    "            text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "            words = text.split()\n",
    "            words = [w for w in words if w not in stop_words]\n",
    "            return \" \".join(words)\n",
    "\n",
    "\n",
    "    processed_sentence1 = preprocess_sentence(sentence1)\n",
    "    processed_sentence2 = preprocess_sentence(sentence2)\n",
    "    final_score = 0\n",
    "    \n",
    "    for i in range(1,3):\n",
    "        \n",
    "        nlp = spacy.load('en_core_web_lg' if i == 1 else 'en_core_web_md')\n",
    "\n",
    "        # print(nlp.meta['name'], '\\n')\n",
    "        weighted_score = 0\n",
    "\n",
    "        # Compute Spacy similarity score\n",
    "        doc1 = nlp(processed_sentence1)\n",
    "        doc2 = nlp(processed_sentence2)\n",
    "        spacy_similarity_score = 0\n",
    "        spacy_similarity_score = doc1.similarity(doc2)\n",
    "\n",
    "\n",
    "        # Compute TF-IDF vector representation for each sentence\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        sentence1_tfidf = vectorizer.fit_transform([processed_sentence1])\n",
    "        sentence2_tfidf = vectorizer.transform([processed_sentence2])\n",
    "\n",
    "\n",
    "        # Compute cosine similarity score using TF-IDF vectors\n",
    "        cosine_similarity_score = 0\n",
    "        cosine_similarity_score = cosine_similarity(sentence1_tfidf, sentence2_tfidf)[0][0]\n",
    "\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            tokens1 = word_tokenize(processed_sentence1)\n",
    "            tokens2 = word_tokenize(processed_sentence2)\n",
    "                \n",
    "            # create word embeddings for each string\n",
    "            model = gensim.models.Word2Vec([tokens1, tokens2], min_count=2)\n",
    "            word_vectors = {word: model.wv[word] for word in model.wv.index_to_key}\n",
    "                \n",
    "            # calculate the average vector for each string\n",
    "            vector1 = np.mean([word_vectors[word] for word in tokens1 if word in word_vectors], axis=0)\n",
    "            vector2 = np.mean([word_vectors[word] for word in tokens2 if word in word_vectors], axis=0)\n",
    "                \n",
    "            # calculate cosine similarity between the vectors\n",
    "            sim_score = 0\n",
    "            sim_score = cosine_similarity([vector1], [vector2])[0][0]\n",
    "            # print(\"ends within try\")\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            sim_score = 0\n",
    "            # print(\"except\")\n",
    "            sim_score = weighted_score\n",
    "\n",
    "\n",
    "        tokens1 = set(nltk.word_tokenize(sentence1.lower()))\n",
    "        tokens2 = set(nltk.word_tokenize(sentence2.lower()))\n",
    "\n",
    "        jaccard_similarity_score = 0\n",
    "        jaccard_similarity_score = len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))\n",
    "\n",
    "\n",
    "        # Weighted average of the similarity scores\n",
    "        weighted_score = (0.5 * spacy_similarity_score) + (0.3 * jaccard_similarity_score) + (0.2 * cosine_similarity_score)\n",
    "\n",
    "\n",
    "        # Compute the average similarity score\n",
    "        average_similarity_score = 0\n",
    "        average_similarity_score = spacy_similarity_score * .15 + ( jaccard_similarity_score + cosine_similarity_score + ( weighted_score + sim_score ) * .15 ) / 2\n",
    "        final_score += average_similarity_score\n",
    "        # # Print the similarity scores\n",
    "        # print(\"Spacy similarity score:\", spacy_similarity_score)\n",
    "        # print(\"Cosine similarity score:\", cosine_similarity_score)\n",
    "        # print(\"Jaccard similarity score:\",jaccard_similarity_score)\n",
    "        # print(\"Weighted similarity score:\", weighted_score)\n",
    "        # print(\"Gensim similarity score:\", sim_score)\n",
    "        # # print(\"average \",average_similarity_score)\n",
    "        # print(\"\\n\\nAnswer score is :\",average_similarity_score,\"\\n\\n\")\n",
    "    res = ceil((final_score/2)*100)\n",
    "    # print(\"Simalarity score is :\",0 if ceil((final_score/2)*100) < 50 else (100 if ceil((final_score/2)*100) > 100 else ceil((final_score/2)*100)))\n",
    "    return 0 if res < 50 else (100 if res > 100 else res) \n",
    "    # return average_similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer score : 87\n"
     ]
    }
   ],
   "source": [
    " # Define two sample sentences\n",
    "# sentence1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "# sentence2 = \"A quick brown dog jumps over the lazy cat\"\n",
    "# sentence1 = \"Machine Learning (ML) is a subfield of Artificial Intelligence (AI) that involves the development of algorithms and models that enable computers to learn from data and make predictions or decisions without being explicitly programmed. In other words, ML involves training a machine to identify patterns in data and use those patterns to make predictions or decisions.\"\n",
    "# sentence2 = \"Machine Learning can also refer to the process of training a machine learning model on a dataset to make predictions or decisions. This involves selecting an appropriate algorithm, preparing the data, and tuning the parameters of the model to achieve the desired level of accuracy. The ultimate goal of ML is to create models that can generalize well to new, unseen data and make accurate predictions or decisions in real-world scenarios.\"\n",
    "# sentence1 = \"Machine Learning (ML) is a subfield of Artificial Intelligence (AI) that involves the development of algorithms and models that enable computers to learn from data and make predictions or decisions without being explicitly programmed. In other words, ML involves training a machine to identify patterns in data and use those patterns to make predictions or decisions.\"\n",
    "# sentence2 = \"Environmental Studies(EVS full form) is the study of the environment, which refers to the surroundings in which all live. Everything that directly or indirectly promotes human survival is considered part of the environment. Plants, animals, and soil, water, and air are examples of living and non-living components. The living components interact with their surroundings and adjust to the changing conditions.\"\n",
    "# sentence1 = \"computer science is a type of artificial intelligence (AI) that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. Machine learning algorithms use historical data as input to predict new output values.\"\n",
    "# sentence2 = \"computer science is the study of computers and computational systems. Unlike electrical and computer engineers, computer scientists deal mostly with software and software systems; this includes their theory, design, development, and application.\"\n",
    "# sentence1 = \"The bright and radiant sun shone down upon the lush green fields filling them with warmth and life\"\n",
    "# sentence2 = \"The brilliant and glowing star cast its brilliant light onto the verdant meadows imbuing them with vitality and heat\"\n",
    "# sentence1 = \"Social media has revolutionized the way we communicate and interact with each other.\"\n",
    "# sentence2 = \"The rise of social media has transformed the landscape of communication and interpersonal relationships.\"\n",
    "# sentence1 =  \"Artificial intelligence (AI) is a field of computer science that focuses on creating machines that can perform tasks that typically require human intelligence.\"\n",
    "# sentence2 = \"AI involves developing algorithms and models that enable machines to perform intelligent tasks, such as natural language processing and computer vision.\"\n",
    "# sentence1 = \"The internet has become an essential tool for communication, research, and commerce in the modern era.\"\n",
    "# sentence2 = \"In today's society, the internet plays a critical role in connecting people, providing information, and facilitating economic transactions.\"\n",
    "sentence1 = \"The Earth's atmosphere consists of several layers that protect the planet from harmful radiation and provide the air we breathe.\"\n",
    "sentence2 = \"The Earth's atmosphere is composed of several layers that help shield the planet from harmful cosmic rays and provide the necessary gases for life to thrive.\"\n",
    "#sentence1 = \"Mobile devices have transformed the way we consume media, with many people now using smartphones and tablets to watch videos, listen to music, and browse the web.\"\n",
    "# sentence2 = \"The widespread adoption of mobile devices has led to a shift in how we access and consume media, with many people now preferring to watch videos, listen to music, and browse the internet on their smartphones and tablets.\"\n",
    "res = getScore(sentence1,sentence2)\n",
    "print(\"Answer score :\",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "027efd7095115064582efab50921964d678032cf36f1ec215c675cf4ffc5e6b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
